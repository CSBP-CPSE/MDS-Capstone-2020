---
title: "PHU stats"
author: "Sofia Bahmutsky"
date: "25/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(FactoMineR)
library(factoextra)

# load the data
data <- read.csv("/Users/Sofia/Desktop/data-599-capstone-statistics-canada/data/PMD-en/PHU_FINAL_prop.csv")

str(data)
percents <- c('copd.percent', 'asthma.percent', 'smokers.percent', 'hbp.percent')
data[,percents] <- lapply(data[,percents], function(x) as.numeric(gsub("[\\%,]", "", x))/100)

str(data)

names(data)[names(data) == "copd.percent"] <- "copd.prop"
names(data)[names(data) == "asthma.percent"] <- "asthma.prop"
names(data)[names(data) == "hbp.percent"] <- "hbp.prop"
names(data)[names(data) == "smokers.percent"] <- "smokers.prop"

# dropping unneeded cols
df <- subset(data, select = -c(X, DGUID, Reporting_PHU, fid, DBUID, PRUID_x, CSDUID_x, CMAUID_x, CMAPUID_x, HR_UID, DAUID, lon, lat, transit_na, suppressed))
  
# dropping some cols which may be not make sense in this analysis...(need to check with John i think)
df <- subset(df, select = -c(FATALprop, NOT.RESOLVEDprop, RESOLVEDprop))
df[is.na(df)] <- 0
           
head(df)

library(skimr)
skim(df)
# could try log transformations for the majority of variables.
```

```{r}
# our reposonse variable is totalprop (the proportion of cases in a PHU) the location is the label fro each row. 

# the transit proximity column has missing values. For our purposes they can be changed to zeroes. missing values means that there is no data collected for transit proximity.

# textbook method- subset selection methods

library(glmnet)
x=model.matrix(TOTALprop~.-Location,df)[,-1] 
dim(x)
y <- df$TOTALprop
y <- as.vector(y)



#LASSO
set.seed (1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:23,] 
lasso.coef
```

```{r}
# Ridge Regression
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
ridge.coef=predict(out,type="coefficients",s=bestlam)[1:23,] 
ridge.coef
```


LM
```{r}
library(pls)

pcr.fit=pcr(TOTALprop~.-Location, data=df ,scale=TRUE)

summary(pcr.fit)

test <- lm(TOTALprop ~. -Location, df)
plot(test)

summary(test)
```

trying with transformations and dropping gender columns
```{r}
library(rcompanion)
# Log transformations are better for the inspections data
plotNormalHistogram(df$TOTALprop)

#drop gender cols...
df <- subset(df, select = -c(Location, MALEprop, FEMALEprop, TRANSGENDERprop, OTHERprop, UNKNOWNprop, NEITHERprop, CONTACTprop, TRAVEL.RELATEDprop))
log_df <- log(df+0.00001)
plotNormalHistogram(log_df$TOTALprop)


test_logs <- lm(log_df$TOTALprop ~., log_df)
plot(test_logs)
summary(test_logs)

```


```{r}
set.seed(123)    # seef for reproducibility
library(glmnet)  # for ridge regression
library(dplyr)   # for data cleaning
library(psych)   # for function tr() to compute trace of a matrix

# Center y, X will be standardized in the modelling function
y <- df %>% select(TOTALprop) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- df %>% select(-c(TOTALprop)) %>% as.matrix()


# Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)


# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)


# Best cross-validated lambda
lambda_cv <- lasso_cv$lambda.min
# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(X, y, alpha = 0, lambda = lambda_cv, standardize = TRUE)
y_hat_cv <- predict(model_cv, X)
ssr_cv <- t(y - y_hat_cv) %*% (y - y_hat_cv)
rsq_lasso_cv <- cor(y, y_hat_cv)^2


# See how increasing lambda shrinks the coefficients --------------------------
# Each line shows coefficients for one variables, for different lambdas.
# The higher the lambda, the more the coefficients are shrinked towards zero.
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .7)


```


```{r}
#df <- subset(df, select = -c(Location, MALEprop, FEMALEprop, TRANSGENDERprop, OTHERprop, UNKNOWNprop, NEITHERprop))

# Center y, X will be standardized in the modelling function
y <- df %>% select(TOTALprop) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- df %>% select(-c(TOTALprop)) %>% as.matrix()


# Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)


# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)


# Best cross-validated lambda
lambda_cv <- lasso_cv$lambda.min
# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(X, y, alpha = 1, lambda = lambda_cv, standardize = TRUE)
y_hat_cv <- predict(model_cv, X)
ssr_cv <- t(y - y_hat_cv) %*% (y - y_hat_cv)
rsq_lasso_cv <- cor(y, y_hat_cv)^2


# See how increasing lambda shrinks the coefficients --------------------------
# Each line shows coefficients for one variables, for different lambdas.
# The higher the lambda, the more the coefficients are shrinked towards zero.
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .7)
```